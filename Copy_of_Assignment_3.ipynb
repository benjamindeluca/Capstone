{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjamindeluca/Capstone/blob/main/Copy_of_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AN4g_V-9Ty5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45163d35-7055-44aa-f4cf-70403088488c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is not available\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "# Training will be significantly faster if GPU is available. In Colab, go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "# set the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data:\n",
        "Download the train.zip file from https://www.kaggle.com/c/grasp-and-lift-eeg-detection/data and\n",
        "explore the dataset. This file contains the first 8 series for each subject. (We will be only using\n",
        "train.zip for the project.)\n",
        "There are two files for each subject + series combination:\n",
        "● the *_data.csv files contain the raw 32 channels EEG data (sampling rate 500Hz)\n",
        "● the *_events.csv files contains the ground truth frame-wise labels for all events\n",
        "\n",
        "NOTE: to import the data you have to log into kaggle and create an API token. From there you upload the kaggle.json file that will be downloaded when creating the API token and upload it into the connected google drive account for colab. Then give colab permission to access the files when running the below lines."
      ],
      "metadata": {
        "id": "GIE6OCntWFka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gbhk3dmXsDQ",
        "outputId": "6257e132-b26c-4720-98db-cb9eee0e172c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYSnHGQlqkIb",
        "outputId": "98cf9a80-43b9-44ca-d7a6-82e64a5b2d33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RaPOo89rF1O",
        "outputId": "72f63eb4-78fe-4b4e-d961-65ba70263cb4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "2CbmPDG_rMMU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download grasp-and-lift-eeg-detection --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXQveKKyrUms",
        "outputId": "ca550027-78e0-4936-b650-48496c771b36"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading grasp-and-lift-eeg-detection.zip to /content\n",
            " 99% 1.01G/1.02G [00:11<00:00, 71.0MB/s]\n",
            "100% 1.02G/1.02G [00:12<00:00, 91.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile('grasp-and-lift-eeg-detection.zip', 'r') as zip_ref:\n",
        "    # Extract all files\n",
        "    zip_ref.extractall('grasp-and-lift-eeg-detection')\n",
        "\n",
        "# List the extracted files\n",
        "extracted_files = zip_ref.namelist()\n",
        "\n",
        "display(extracted_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "SFO8oHjqtK1p",
        "outputId": "63858eb9-3dbe-429e-9eb9-0963e74733b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['sample_submission.csv.zip', 'test.zip', 'train.zip']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read data and labels\n",
        "def read_data_and_labels(zip_file):\n",
        "    train_data = {}\n",
        "    test_data = {}\n",
        "    test_labels = {}\n",
        "    train_labels = {}\n",
        "\n",
        "    # Extracted folder name will be the same as the zip file name without the extension\n",
        "    extracted_folder = os.path.splitext(zip_file)[0]\n",
        "\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        # Extract all files\n",
        "        zip_ref.extractall(extracted_folder)\n",
        "\n",
        "    extracted_files = zip_ref.namelist()\n",
        "\n",
        "    for file_name in extracted_files:\n",
        "        if file_name.endswith('_data.csv'):\n",
        "            subject_id, series = file_name.split('_')[:2]\n",
        "            df = pd.read_csv(os.path.join(extracted_folder, file_name))\n",
        "            if series == 'series7' or series == 'series8':\n",
        "                test_data.setdefault(subject_id, []).append(df)\n",
        "            else:\n",
        "                train_data.setdefault(subject_id, []).append(df)\n",
        "\n",
        "        elif file_name.endswith('_events.csv'):\n",
        "            subject_id, series = file_name.split('_')[:2]\n",
        "            labels_df = pd.read_csv(os.path.join(extracted_folder, file_name))\n",
        "\n",
        "            if series == 'series7' or series == 'series8':\n",
        "              test_labels.setdefault(subject_id, []).append(labels_df)\n",
        "            else:\n",
        "              train_labels.setdefault(subject_id, []).append(labels_df)\n",
        "\n",
        "\n",
        "    return train_data, test_data, test_labels, train_labels\n",
        "\n",
        "# Read data and labels\n",
        "train_data, test_data, test_labels, train_labels = read_data_and_labels('grasp-and-lift-eeg-detection/train.zip')\n",
        "\n"
      ],
      "metadata": {
        "id": "F1q3GFzEvpvS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.keys())\n",
        "print(test_data.keys())\n",
        "\n",
        "# Example: Accessing data for subject '1'\n",
        "subject_id = 'train/subj1'\n",
        "train_data_subject_1 = pd.concat(train_data[subject_id], ignore_index=True)\n",
        "test_data_subject_1 = pd.concat(test_data[subject_id], ignore_index=True)\n",
        "test_labels_subject_1 = pd.concat(test_labels[subject_id], ignore_index=True)\n",
        "train_labels_subject_1 = pd.concat(train_labels[subject_id], ignore_index=True)\n",
        "\n",
        "# display(test_labels_subject_1)\n",
        "# Now you have train and test data along with labels for each subject"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pcXwKFax_gP",
        "outputId": "8053b944-eb3b-4373-ef5a-69c1bc967cf0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['train/subj10', 'train/subj11', 'train/subj12', 'train/subj1', 'train/subj2', 'train/subj3', 'train/subj4', 'train/subj5', 'train/subj6', 'train/subj7', 'train/subj8', 'train/subj9'])\n",
            "dict_keys(['train/subj10', 'train/subj11', 'train/subj12', 'train/subj1', 'train/subj2', 'train/subj3', 'train/subj4', 'train/subj5', 'train/subj6', 'train/subj7', 'train/subj8', 'train/subj9'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the data\n",
        "Explore on EEG signal preprocessing methods and make choices on what sort of preprocessing this\n",
        "dataset would need. You can justify your choices in the report"
      ],
      "metadata": {
        "id": "p_eT_tP4XBZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom Dataset Class\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data_dict_raw, labels_dict):\n",
        "        self.data_dict = data_dict_raw\n",
        "        self.labels_dict = labels_dict\n",
        "\n",
        "        # Extract subject IDs and series\n",
        "        self.subject_names = list(data_dict_raw.keys())\n",
        "        self.series = ['series1', 'series2', 'series3', 'series4', 'series5', 'series6','series7', 'series8', 'series9', 'series10', 'series11', 'series12']\n",
        "\n",
        "        # broken\n",
        "        self.subjects = {}\n",
        "        for subject in self.subject_names:\n",
        "          self.subjects.update({str(subject): data_dict_raw[subject]})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subject_names) * len(self.series)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        subject_idx = idx // len(self.series)\n",
        "        series_idx = idx % len(self.series)\n",
        "        subject_id = self.subject_names[subject_idx]\n",
        "        series = self.series[series_idx]\n",
        "\n",
        "        # Get EEG data and labels for the current subject and series\n",
        "        eeg_data = self.subjects[subject_id][series_idx]\n",
        "        labels = self.labels_dict[subject_id]\n",
        "\n",
        "        return eeg_data.values, labels.values\n",
        "\n",
        "    # Need a function to update the data_dict\n",
        "\n",
        "\n",
        "# Create Dataset instances\n",
        "# print(train_data['train/subj10'][1])\n",
        "train_dataset = EEGDataset(train_data, train_labels)\n",
        "test_dataset = EEGDataset(test_data, test_labels)\n",
        "\n",
        "print(train_dataset.subjects['train/subj10'][0])\n",
        "\n",
        "# print(train_dataset.data_dict)"
      ],
      "metadata": {
        "id": "6qHzDGkGiEQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b0901b7-130d-4161-f460-5f07ef90ff48"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           id        Fp1        Fp2          F7         F3  \\\n",
            "0            subj10_series1_0  -4.497380 -29.211153  -59.151118 -26.991991   \n",
            "1            subj10_series1_1  10.257334 -58.665263  -38.470355 -64.551683   \n",
            "2            subj10_series1_2  16.140461 -84.515489  -41.236517 -90.158499   \n",
            "3            subj10_series1_3  12.358437 -90.343431  -63.392971 -97.427322   \n",
            "4            subj10_series1_4   5.988439 -63.986183  -74.776784 -87.126075   \n",
            "...                       ...        ...        ...         ...        ...   \n",
            "262382  subj10_series1_262382  28.341714 -39.437041  -14.427492  92.584966   \n",
            "262383  subj10_series1_262383  46.953302  -9.218532   49.531051  86.017622   \n",
            "262384  subj10_series1_262384  61.373963  25.123945   96.284843  74.320547   \n",
            "262385  subj10_series1_262385  59.464584  33.134911  100.920553  56.044810   \n",
            "262386  subj10_series1_262386  41.813923  11.084058   66.367898  31.829118   \n",
            "\n",
            "               Fz         F4         F8         FC5        FC1  ...  \\\n",
            "0        5.065385   3.209991  69.830905  -13.001308  -8.604097  ...   \n",
            "1       -4.766212 -16.186504  79.114297  -43.361650 -27.979986  ...   \n",
            "2      -10.920965 -27.699987  62.189268  -55.394838 -41.703829  ...   \n",
            "3      -12.520526 -25.942664  27.064680  -42.127103 -48.566359  ...   \n",
            "4      -12.264553 -13.407760   7.132135  -11.454193 -50.595667  ...   \n",
            "...           ...        ...        ...         ...        ...  ...   \n",
            "262382   7.179442  11.623865   4.375875  153.995720  42.843640  ...   \n",
            "262383  19.231758  30.694512  31.916759  120.801965  41.880315  ...   \n",
            "262384  31.499423  44.838218  56.733754   77.963836  34.752405  ...   \n",
            "262385  32.057818  41.254257  51.119939   51.125924  23.782126  ...   \n",
            "262386  19.776105  21.366515  13.751701   42.505272  11.475189  ...   \n",
            "\n",
            "               P7         P3         Pz          P4          P8        PO9  \\\n",
            "0       -7.985887  29.246840  47.067967   83.506346   49.140766 -20.352309   \n",
            "1        3.731932  23.463502  53.938344   78.132852   32.586492  20.892750   \n",
            "2       16.759281  19.262155  53.748704   73.993946   25.253877  33.135837   \n",
            "3       23.363656  15.950168  49.055371   73.713872   32.361715   7.504686   \n",
            "4       16.365312  13.276949  49.012469   79.409364   51.407432 -37.175356   \n",
            "...           ...        ...        ...         ...         ...        ...   \n",
            "262382 -38.573758 -17.890328 -52.380005 -121.504222 -145.984273 -68.883026   \n",
            "262383 -28.776415 -20.664543 -42.010232 -101.728721 -122.522172 -66.736314   \n",
            "262384 -10.232685 -21.446897 -30.323887  -79.703446  -86.604903 -57.931194   \n",
            "262385  10.522131 -13.995789 -27.823401  -64.917191  -62.895222 -43.104745   \n",
            "262386  29.096200   1.315995 -35.190111  -58.351473  -57.917054 -25.428793   \n",
            "\n",
            "               O1         Oz          O2        PO10  \n",
            "0       29.480804  40.002275   60.122510    5.341084  \n",
            "1       49.306323  49.147069   28.981590   -7.422710  \n",
            "2       51.476489  42.090482    9.170991  -15.622194  \n",
            "3       34.032143  16.357024    5.288734  -12.415391  \n",
            "4       11.619773 -14.312317   13.526148    8.347411  \n",
            "...           ...        ...         ...         ...  \n",
            "262382 -65.105676 -88.114881 -153.071477 -119.838689  \n",
            "262383 -51.064229 -67.168167 -131.724817 -120.826036  \n",
            "262384 -32.711084 -46.576229 -101.192875  -97.892281  \n",
            "262385 -19.793974 -37.890246  -78.302598  -70.926363  \n",
            "262386 -14.313281 -41.213904  -67.052032  -52.980603  \n",
            "\n",
            "[262387 rows x 33 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering the Data\n"
      ],
      "metadata": {
        "id": "ZXR0aQrGfd9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "\n",
        "# Making a copy of the dataset so as the maintain the original\n",
        "filtered_train = train_dataset\n",
        "filtered_test = test_dataset\n",
        "\n",
        "def filter_eeg_dataset(EEGDataset, f_low, f_high, order, type, fs):\n",
        "  b, a = signal.butter(order, [f_low, f_high], btype=type, fs=fs)\n",
        "\n",
        "  subject_names = EEGDataset.subject_names\n",
        "\n",
        "  # For each Subject in EEG Dataset\n",
        "  for subject in subject_names:\n",
        "    # For each Series in Subject\n",
        "    for series in range(6):\n",
        "      try:\n",
        "        data_trial = EEGDataset.subjects[subject][series]\n",
        "        # For each Column in series and subject\n",
        "        # Not the first non-numerical columns\n",
        "        for col_name in data_trial.columns:\n",
        "\n",
        "          if col_name != \"id\":\n",
        "            filtered_data = signal.filtfilt(b, a, data_trial[col_name])\n",
        "\n",
        "            # Replace data\n",
        "            EEGDataset.subjects[subject][series][col_name] = filtered_data\n",
        "\n",
        "\n",
        "      # Breaking when we run out of series\n",
        "      except IndexError as e:\n",
        "        print(str(subject) + \" caps at \"+ str(series))\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# System Params\n",
        "order = 4\n",
        "sampling_frequency = 500\n",
        "\n",
        "# High and Low Pass Filter\n",
        "f_low = 1\n",
        "f_high = 100\n",
        "f_type = 'bandpass'\n",
        "\n",
        "filter_eeg_dataset(filtered_train, f_low, f_high, order, f_type, sampling_frequency)\n",
        "filter_eeg_dataset(filtered_test, f_low, f_high, order, f_type, sampling_frequency)\n",
        "\n",
        "# Bandreject\n",
        "f_low = 49\n",
        "f_high = 51\n",
        "f_type = 'bandstop'\n",
        "\n",
        "filter_eeg_dataset(filtered_train, f_low, f_high, order, f_type, sampling_frequency)\n",
        "filter_eeg_dataset(filtered_test, f_low, f_high, order, f_type, sampling_frequency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2k5pnDTffke",
        "outputId": "e9b81edd-fcec-41dd-d1a3-142f343b67de"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train/subj10 caps at 2\n",
            "train/subj11 caps at 2\n",
            "train/subj12 caps at 2\n",
            "train/subj1 caps at 2\n",
            "train/subj2 caps at 2\n",
            "train/subj3 caps at 2\n",
            "train/subj4 caps at 2\n",
            "train/subj5 caps at 2\n",
            "train/subj6 caps at 2\n",
            "train/subj7 caps at 2\n",
            "train/subj8 caps at 2\n",
            "train/subj9 caps at 2\n",
            "train/subj10 caps at 2\n",
            "train/subj11 caps at 2\n",
            "train/subj12 caps at 2\n",
            "train/subj1 caps at 2\n",
            "train/subj2 caps at 2\n",
            "train/subj3 caps at 2\n",
            "train/subj4 caps at 2\n",
            "train/subj5 caps at 2\n",
            "train/subj6 caps at 2\n",
            "train/subj7 caps at 2\n",
            "train/subj8 caps at 2\n",
            "train/subj9 caps at 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction\n"
      ],
      "metadata": {
        "id": "lKDQGVI6fgLE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiVSDcvjfiWT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Neural Network"
      ],
      "metadata": {
        "id": "7UYzd95wYvY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader instances\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(filtered_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(filtered_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class EEGModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(EEGModel, self).__init__()\n",
        "        # Define your neural network architecture here\n",
        "        # Example:\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "        self.fc3 = nn.Conv1d()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass of your network\n",
        "        # Example:\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class Conv1DModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels, kernel_size):\n",
        "        super(Conv1DModel, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=input_size, out_channels=num_channels, kernel_size=kernel_size),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),  # Max pooling layer\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels*2, kernel_size=kernel_size),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),  # Max pooling layer\n",
        "            nn.Conv1d(in_channels=num_channels*2, out_channels=num_channels*4, kernel_size=kernel_size),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)   # Max pooling layer\n",
        "        )\n",
        "        # Calculate output size after convolutions\n",
        "        self.fc_input_size = num_channels*4 * ((input_size - 2*(kernel_size-1)) // 2**3)\n",
        "        # Fully connected layer\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(-1, self.fc_input_size)  # Reshape for fully connected layers\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# We could make a bi-directional RNN, which violates real-time detection, but can analyse other recorded test sets"
      ],
      "metadata": {
        "id": "6yw0xnCxYht7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nnspt"
      ],
      "metadata": {
        "id": "L87Gk47ukTE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the input size from the train_loader\n",
        "# for inputs, _ in train_loader:\n",
        "#     input_size = inputs.shape[1]  # Get the number of features from the shape of the input data\n",
        "#     break  # Break after extracting the first batch\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# Define the neural network model with the correct input size\n",
        "hidden_size = 64  # Example hidden size\n",
        "num_classes = 6  # Example number of classes\n",
        "\n",
        "# model = Conv1DModel(input_size, hidden_size, num_classes)\n",
        "epochs = 20\n",
        "\n",
        "model = Conv1DModel(input_size=32, output_size=6, num_channels=64, kernel_size = 200)\n",
        "model.to(device)\n",
        "optimiser = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "sheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, epochs*len(train_dataloader))\n",
        "\n"
      ],
      "metadata": {
        "id": "INsqQaU5ZmWw",
        "outputId": "b1383813-f2c7-4287-dc89-d1c5cd528884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'nnspt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f16ad5b40608>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     break  # Break after extracting the first batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnnspt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nnspt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}